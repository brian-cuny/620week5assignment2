{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Week 5 Assignment 2\n",
    "##Niteen Kumar, Alexander Low, Jagruti Solao, Brian Weinfeld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we are using a corpus of websites that have been labeled to identify their topics. The four categories are 'Band', 'BioMedical', 'Goats' and 'Sheep'. There are approximately 60 websites (html only) for each category.\n",
    "\n",
    "We will use the nltk package to train a Naive Bayes Classifier and a Decision Tree Classifier based on the presence of particular words in each of the websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(topic, i):\n",
    "    with open(f'/home/brian/Desktop/DataScience/620/620week5assignment2/Train/{topic}/{i}', 'rb') as response:\n",
    "        html = response.read()\n",
    "        raw = BeautifulSoup(html, 'html.parser')\n",
    "        tokens = nltk.word_tokenize(raw.get_text())\n",
    "        tokens = set(w.lower() for w in tokens if w.isalpha())\n",
    "        tokens = tokens.difference(stopwords.words('english'))\n",
    "\n",
    "        features = {}\n",
    "        for t in tokens:\n",
    "            features[t] = True\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function takes a specific file as it's parameter and creates a feature set based on that website. This is done by parsing the html, tokenizing the results, removing non-words and then keeping only the important words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_read():\n",
    "    featuresets_bands = [(feature('Bands', i), 'Bands') for i in range(1,61)]\n",
    "    featuresets_bio = [(feature('BioMedical', i), 'BioMedical') for i in range(1,61)]\n",
    "    featuresets_goats = [(feature('Goats', i), 'Goats') for i in range(1,58)]\n",
    "    featuresets_sheeps = [(feature('Sheep', i), 'Sheep') for i in range(1,57)]\n",
    "\n",
    "    full_set = featuresets_bands + featuresets_bio + featuresets_goats + featuresets_sheeps\n",
    "    random.shuffle(full_set)\n",
    "\n",
    "    file_name = 'data'\n",
    "    file_object = open(file_name, 'wb')\n",
    "    pickle.dump(full_set, file_object)\n",
    "    file_object.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function creates the featureset for each website, combines all the information into one list and then writes the file out to simplify future running of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6956521739130435"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'data'\n",
    "file_object = open(file_name, 'rb')\n",
    "full_set = pickle.load(file_object)\n",
    "\n",
    "random.shuffle(full_set)\n",
    "size = int(len(full_set) * 0.1)\n",
    "\n",
    "train_set, test_set = full_set[size:], full_set[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n                    full = True            Bands : Goats  =     37.0 : 1.0\n                 excerpt = None            Goats : Bands  =     35.0 : 1.0\n                   music = True            Bands : Goats  =     32.1 : 1.0\n                   songs = True            Bands : Goats  =     27.1 : 1.0\n                    goat = True            Goats : Sheep  =     24.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes Classifier has achieved an accuracy of nearly 70%. The most common identifying words are listed as well. It seems likely that the most difficult identification was the result of Goat vs. Sheep. While the other topics are very different and would likely use different language, the Goat and Sheep websites likely use very similar language. This would explain why the word 'goat' is such a code identifying feature between the two typs of sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9565217391304348"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_classifier = nltk.classify.DecisionTreeClassifier.train(train_set)\n",
    "nltk.classify.accuracy(tree_classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decision Tree Classifier was able to obtain an impressive 95% accuracy. This is likely due to the small size of the data set. It would be interesting to see how the tree classifier would handle a larger data set with more topics.\n",
    "\n",
    "As an added challenge, it would be interesting to add some additional \"challenge\" websites. For example, a site that talks about medical care for Sheep or a site that talks about the band \"Goat\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we were able to successfully use the nltk package to train a classifier to identify diferent types of websites based on the words in that site. We were able to achieve an impressive level of accuracy with the potential for extending the project in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
